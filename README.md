
# Quantum Multimodality: Multi-Stage Compositional Concept Generalization

This repository contains the code for experiments on **grounded compositional concept generalization**
using **quantum, quantum-inspired, and classical multimodal models**.

We study whether relational knowledge learned from data is represented **compositionally** and
whether it generalizes systematically across tasks and distributions.

The work follows a **multi-stage training framework** that explicitly separates:
1. object-level grounding, and
2. relational learning,

allowing a controlled analysis of relational competence versus compositional representation.

---

## Overview of the Framework

### Stage 1: Single-Object Training (Object Grounding)

In the first stage, the model is trained on **single-object image–caption pairs**
corresponding to the four geometric shapes:

> **cube, cone, cylinder, sphere**

The goal of this stage is to learn **stable object-level representations** independently
of any relational context.

- Task: image–caption alignment
- Objective: supervised contrastive loss
- Images: frozen CLIP embeddings
- Text: quantum or classical compositional encoders

The learned object representations are **frozen and reused** in later stages.

---

### Stage 2: Relational Training (Compositional Learning)

In the second stage, the model is exposed to **relational scenes** containing two objects
and a spatial relation (e.g. *“cube left cone”*).

Object-level parameters learned in Stage 1 are frozen, and only **relation-specific
parameters** are trained.

We study relational learning under two complementary regimes:

#### 1. Image–Caption Alignment (Matching)
- The model distinguishes correct image–caption pairs from distractors
  generated by swapping relations (*left ↔ right*).
- This task probes whether relational structure is **implicitly encoded**
  in the geometry of the learned embedding space.

#### 2. Binary Relation Classification
- The model is trained with explicit supervision to classify relations (*left vs right*).
- After training, the model is evaluated on image–caption matching **without further training**.
- This allows us to test whether explicitly learned relations transfer to
  compositional similarity-based evaluation.

---

## Repository Structure

```text
Quantum-Multimodality/
├── data/               # Dataset splits (single-object and relational)
├── src/                # Core source code
│   ├── models/         # Quantum and classical models
│   ├── training/       # Training scripts for each stage
│   └── evaluation/     # Matching and classification evaluation
├── experiments/        # Experiment-specific configurations
├── results/            # Tables, figures, and logs
├── scripts/            # Convenience run scripts
├── notebooks/          # Analysis notebooks
└── paper/              # PDF and paper figures

## Environment Setup

To install dependencies:

```bash
pip install -r requirements.txt

---

## Multi-Stage Training (Single → Relational)

This repository implements the **multi-stage training protocol** used in the paper.

The goal is to first learn **shape-specific quantum parameters** from single-object data,
and then transfer (freeze) these parameters when training on **relational captions**.

---

### Stage 1: Single-Object Image–Caption Alignment (Angle Encoding)

In Stage 1, the model is trained on **single objects** (cube, cone, cylinder, sphere)
using the **image–caption alignment task** with angle encoding.

This stage learns quantum parameters associated with **shape words only**.

Run Stage 1 training with:

```bash
python -m src.training.single_object.train_angle

After training, the learned parameters are saved to:
pretrained/single_object_angle_params.txt

###Stage 2: Relational Image–Caption Alignment (Multi-Stage)

In Stage 2, the model is trained on **relational captions**, such as:

- `cone left cube`
- `sphere right cylinder`

The training follows a **multi-stage transfer learning setup**:

- Shape parameters learned in **Stage 1 (single-object training)** are **loaded and frozen**
- Only **relational parameters** (e.g. *left*, *right*) are optimised

### Running multi-stage relational training

```bash
python -m src.training.relational.train_angle_alignment_multistage
The parameter loading and freezing logic is implemented in:
src/models/quantum/relational/freezing.py

## Notes

- This repository focuses on **image–caption alignment**.
- The same **parameter-freezing strategy** can be applied to **binary relation classification**.


