
# Quantum Multimodality: Multi-Stage Compositional Concept Generalization

This repository contains the code for experiments on **grounded compositional concept generalization**
using **quantum, quantum-inspired, and classical multimodal models**.

We study whether relational knowledge learned from data is represented **compositionally** and
whether it generalizes systematically across tasks and distributions.

The work follows a **multi-stage training framework** that explicitly separates:
1. object-level grounding, and
2. relational learning,

allowing a controlled analysis of relational competence versus compositional representation.

---

## Overview of the Framework

### Stage 1: Single-Object Training (Object Grounding)

In the first stage, the model is trained on **single-object image–caption pairs**
corresponding to the four geometric shapes:

> **cube, cone, cylinder, sphere**

The goal of this stage is to learn **stable object-level representations** independently
of any relational context.

- Task: image–caption alignment
- Objective: supervised contrastive loss
- Images: frozen CLIP embeddings
- Text: quantum or classical compositional encoders

The learned object representations are **frozen and reused** in later stages.

---

### Stage 2: Relational Training (Compositional Learning)

In the second stage, the model is exposed to **relational scenes** containing two objects
and a spatial relation (e.g. *“cube left cone”*).

Object-level parameters learned in Stage 1 are frozen, and only **relation-specific
parameters** are trained.

We study relational learning under two complementary regimes:

#### 1. Image–Caption Alignment (Matching)
- The model distinguishes correct image–caption pairs from distractors
  generated by swapping relations (*left ↔ right*).
- This task probes whether relational structure is **implicitly encoded**
  in the geometry of the learned embedding space.

#### 2. Binary Relation Classification
- The model is trained with explicit supervision to classify relations (*left vs right*).
- After training, the model is evaluated on image–caption matching **without further training**.
- This allows us to test whether explicitly learned relations transfer to
  compositional similarity-based evaluation.

---

## Repository Structure

```text
Quantum-Multimodality/
├── data/               # Dataset splits (single-object and relational)
├── src/                # Core source code
│   ├── models/         # Quantum and classical models
│   ├── training/       # Training scripts for each stage
│   └── evaluation/     # Matching and classification evaluation
├── experiments/        # Experiment-specific configurations
├── results/            # Tables, figures, and logs
├── scripts/            # Convenience run scripts
├── notebooks/          # Analysis notebooks
└── paper/              # PDF and paper figures

## Environment Setup

To install dependencies:

```bash
pip install -r requirements.txt

